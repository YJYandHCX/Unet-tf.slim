{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DATA\n",
    "import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yanjianyu/mine/tensorflow/segementation/cat and dog/Net.py:69: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "data = DATA.CADF()\n",
    "net = Net.Unet(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.create_global_step()#创建全局变量\n",
    "learning_rate = 0.01\n",
    "sess = tf.Session()\n",
    "variable_to_restore = tf.global_variables()\n",
    "saver = tf.train.Saver(variable_to_restore)#初始化\n",
    "summary_op = tf.summary.merge_all()#将之前定义的所有summary整合在一起\n",
    "writer = tf.summary.FileWriter(\"./Summ/\",sess.graph,flush_secs=60)#创建一个FileWrite的类对象，并将计算图写入文件\n",
    "tf.summary.scalar('learning_rate',learning_rate)\n",
    "train_step = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(net.total_loss, global_step=global_step)#使用随机梯度下降算法，使参数沿着 梯度的反方向，即总损失减小的方向移动，实现更新参数\n",
    "with tf.control_dependencies([train_step]):\n",
    "    #此函数指定某些操作执行的依赖关系，指执行完train_step，在执行train_op；\n",
    "    #而tf.no_op表示执行完 train_step, variable_averages_op 操作之后什么都不做\n",
    "    train_op = tf.no_op(name='train')\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), loss on training batch is 0.845976.\n",
      "After 0 training step(s), loss on training batch is 0.845976.\n",
      "After 10 training step(s), loss on training batch is 0.724855.\n",
      "After 20 training step(s), loss on training batch is 0.80394.\n",
      "After 30 training step(s), loss on training batch is 0.592781.\n",
      "After 40 training step(s), loss on training batch is 0.715635.\n",
      "After 50 training step(s), loss on training batch is 0.756856.\n",
      "After 60 training step(s), loss on training batch is 0.649883.\n",
      "After 70 training step(s), loss on training batch is 0.792912.\n",
      "After 80 training step(s), loss on training batch is 0.627614.\n",
      "After 90 training step(s), loss on training batch is 0.933661.\n",
      "After 100 training step(s), loss on training batch is 0.677244.\n",
      "After 100 training step(s), loss on training batch is 0.677244.\n",
      "After 110 training step(s), loss on training batch is 0.660727.\n",
      "After 120 training step(s), loss on training batch is 0.616166.\n",
      "After 130 training step(s), loss on training batch is 0.534977.\n",
      "After 140 training step(s), loss on training batch is 0.607879.\n",
      "After 150 training step(s), loss on training batch is 0.708369.\n",
      "After 160 training step(s), loss on training batch is 0.669237.\n",
      "After 170 training step(s), loss on training batch is 0.720201.\n",
      "After 180 training step(s), loss on training batch is 0.637519.\n",
      "After 190 training step(s), loss on training batch is 0.837716.\n",
      "After 200 training step(s), loss on training batch is 0.647421.\n",
      "After 200 training step(s), loss on training batch is 0.647421.\n",
      "After 210 training step(s), loss on training batch is 0.636183.\n",
      "After 220 training step(s), loss on training batch is 0.570704.\n",
      "After 230 training step(s), loss on training batch is 0.518342.\n",
      "After 240 training step(s), loss on training batch is 0.569993.\n",
      "After 250 training step(s), loss on training batch is 0.67654.\n",
      "After 260 training step(s), loss on training batch is 0.653956.\n",
      "After 270 training step(s), loss on training batch is 0.677144.\n",
      "After 280 training step(s), loss on training batch is 0.664434.\n",
      "After 290 training step(s), loss on training batch is 0.786606.\n",
      "After 300 training step(s), loss on training batch is 0.631967.\n",
      "After 300 training step(s), loss on training batch is 0.631967.\n",
      "After 310 training step(s), loss on training batch is 0.606007.\n",
      "After 320 training step(s), loss on training batch is 0.544636.\n",
      "After 330 training step(s), loss on training batch is 0.50628.\n",
      "After 340 training step(s), loss on training batch is 0.577412.\n",
      "After 350 training step(s), loss on training batch is 0.662797.\n",
      "After 360 training step(s), loss on training batch is 0.644274.\n",
      "After 370 training step(s), loss on training batch is 0.666642.\n",
      "After 380 training step(s), loss on training batch is 0.64942.\n",
      "After 390 training step(s), loss on training batch is 0.756379.\n",
      "After 400 training step(s), loss on training batch is 0.623846.\n",
      "After 400 training step(s), loss on training batch is 0.623846.\n",
      "After 410 training step(s), loss on training batch is 0.592645.\n",
      "After 420 training step(s), loss on training batch is 0.537269.\n",
      "After 430 training step(s), loss on training batch is 0.498762.\n",
      "After 440 training step(s), loss on training batch is 0.582509.\n",
      "After 450 training step(s), loss on training batch is 0.750943.\n",
      "After 460 training step(s), loss on training batch is 0.622126.\n",
      "After 470 training step(s), loss on training batch is 0.648582.\n",
      "After 480 training step(s), loss on training batch is 0.645492.\n",
      "After 490 training step(s), loss on training batch is 0.723304.\n",
      "After 500 training step(s), loss on training batch is 0.613902.\n",
      "After 500 training step(s), loss on training batch is 0.613902.\n",
      "After 510 training step(s), loss on training batch is 0.585293.\n",
      "After 520 training step(s), loss on training batch is 0.536384.\n",
      "After 530 training step(s), loss on training batch is 0.482718.\n",
      "After 540 training step(s), loss on training batch is 0.565067.\n",
      "After 550 training step(s), loss on training batch is 0.684271.\n",
      "After 560 training step(s), loss on training batch is 0.604421.\n",
      "After 570 training step(s), loss on training batch is 0.624004.\n",
      "After 580 training step(s), loss on training batch is 0.635624.\n",
      "After 590 training step(s), loss on training batch is 0.690139.\n",
      "After 600 training step(s), loss on training batch is 0.55043.\n",
      "After 600 training step(s), loss on training batch is 0.55043.\n",
      "After 610 training step(s), loss on training batch is 0.573945.\n",
      "After 620 training step(s), loss on training batch is 0.51258.\n",
      "After 630 training step(s), loss on training batch is 0.469572.\n",
      "After 640 training step(s), loss on training batch is 0.551895.\n",
      "After 650 training step(s), loss on training batch is 0.663663.\n",
      "After 660 training step(s), loss on training batch is 0.55246.\n",
      "After 670 training step(s), loss on training batch is 0.702758.\n",
      "After 680 training step(s), loss on training batch is 0.641782.\n",
      "After 690 training step(s), loss on training batch is 0.653915.\n",
      "After 700 training step(s), loss on training batch is 0.576611.\n",
      "After 700 training step(s), loss on training batch is 0.576611.\n"
     ]
    }
   ],
   "source": [
    "for i in range (1001):\n",
    "    xs,ys = data.batch_prepare(i)\n",
    "    feed_dict = {net.images:xs,net.labels:ys} \n",
    "    if i%10==0:\n",
    "        summary_str,loss_value,_,step = sess.run([summary_op,net.total_loss,\n",
    "                                                        train_op,global_step],feed_dict = feed_dict)\n",
    "        writer.add_summary(summary_str,step)\n",
    "        print(\"After %d training step(s), loss on training \"\n",
    "                      \"batch is %g.\" % (step, loss_value))\n",
    "    else :\n",
    "        summary_str,loss_value,_,step = sess.run([summary_op,net.total_loss,train_op,global_step],\n",
    "                                                 feed_dict = feed_dict)\n",
    "    if i % 100 == 0:\n",
    "        print(\"After %d training step(s), loss on training \"\n",
    "                      \"batch is %g.\" % (step, loss_value))\n",
    "        saver.save(sess,\"./save/model.ckpt\",global_step)\n",
    "print(\"training is ending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
